{"componentChunkName":"component---src-templates-tag-js","path":"/tag/engineering/","result":{"data":{"ghostTag":{"slug":"engineering","name":"Engineering","visibility":"public","feature_image":null,"description":"The SDV engineering team is serving a global, open source user base. In our engineering blog, we highlight engineering challenges and design decisions we've made in support of our community.","meta_title":null,"meta_description":null},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__61e841116361ff003b9ca712","title":"Building the Unique Combinations Constraint in the SDV","slug":"building-unique-combinations","featured":false,"feature_image":"https://sdv.ghost.io/content/images/2022/01/Banner-UC.png","excerpt":"Sometimes, you want to limit the amount of permutations in your synthetic data. Explore the strategies we used for enforcing this kind of logic.","custom_excerpt":"Sometimes, you want to limit the amount of permutations in your synthetic data. Explore the strategies we used for enforcing this kind of logic.","visibility":"public","created_at_pretty":"19 January, 2022","published_at_pretty":"25 January, 2022","updated_at_pretty":"26 January, 2022","created_at":"2022-01-19T11:49:21.000-05:00","published_at":"2022-01-25T13:25:20.000-05:00","updated_at":"2022-01-26T17:55:37.000-05:00","meta_title":"Building Unique Combinations","meta_description":"Sometimes, you want to limit the amount of permutations in your synthetic data. Explore the strategies we used for enforcing this kind of logic.","og_description":null,"og_image":null,"og_title":null,"twitter_description":"Sometimes, you want to limit the amount of permutations in your synthetic data. Explore the strategies we used for enforcing this kind of logic.","twitter_image":null,"twitter_title":"Building Unique Combinations","authors":[{"name":"Neha Patki","slug":"neha","bio":"Neha first created the SDV for her Master's thesis at MIT and also has experience in Product Management from Google. She is excited to use her expertise to build a great SDV user experience.","profile_image":"https://sdv.ghost.io/content/images/2021/05/Neha_Patki--1-.jpg","twitter":"@n4atki","facebook":null,"website":"https://www.linkedin.com/in/nehapatki/"}],"primary_author":{"name":"Neha Patki","slug":"neha","bio":"Neha first created the SDV for her Master's thesis at MIT and also has experience in Product Management from Google. She is excited to use her expertise to build a great SDV user experience.","profile_image":"https://sdv.ghost.io/content/images/2021/05/Neha_Patki--1-.jpg","twitter":"@n4atki","facebook":null,"website":"https://www.linkedin.com/in/nehapatki/"},"primary_tag":{"name":"Engineering","slug":"engineering","description":"The SDV engineering team is serving a global, open source user base. In our engineering blog, we highlight engineering challenges and design decisions we've made in support of our community.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Engineering","slug":"engineering","description":"The SDV engineering team is serving a global, open source user base. In our engineering blog, we highlight engineering challenges and design decisions we've made in support of our community.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"By default, a machine learning model (ML) may not always learn the deterministic\nrules in your dataset. We've previously explored how the SDV allows user to \ninput their logic [https://sdv.dev/blog/eng-sdv-constraints/] using constraints.\nWith constraints, an SDV model produces logically correct data 100% of the time.\n\nWhile an end user might expect the constraint to \"just work,\" engineering this\nfunctionality requires some creative techniques. In this article, we'll describe\nthe techniques we used to build the UniqueCombinations constraint. You can also\nfollow along in our notebook\n[https://colab.research.google.com/drive/1bY8y6m7-CjTxWDepw32-ZT3Ubb9RGK5F?usp=sharing]\n.\n\n!pip install sdv==0.13.1\n\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nWhat is a Unique Combinations Constraint?\nUsers frequently encounter logical constraints on the permutations -- mixing &\nmatching -- that are allowed in synthetic data.\n\nTo illustrate this, let's use the world_v1 dataset from the SDV tabular dataset\ndemos. This simple dataset describes the population of different cities around\nthe world.\n\nfrom sdv.demo import load_tabular_demo\n\ndata = load_tabular_demo('world_v1')\ndata = data.drop(['add_numerical'], axis=1) # not needed for this demo\ndata.head()\n\nRelationship between Name, CountryCode and District\n\nLooking at the data, we can observe that there is a special relationship between\nthe Name of the city, its CountryCode and its geographical District: When\ngenerating synthetic data, the model should not blindly mix-and-match these\nvalues. Instead, it should reference the real data to verify whether the\ncombination is valid. This is called a UniqueCombinations constraint.\n\nFor example, take a particular city, like Cambridge, which appears 3 times in\nour dataset.\n\ndata[data.Name == 'Cambridge']\n\nThe constraint states that Cambridge should only ever appear with GBR (England), \nCAN (Ontario) or USA (Massachusetts). It is invalid if it appears in any other\nregion -- for eg. Cambridge, France.\n\nHow does the SDV handle a Unique Combination out-of-the-box?\n\nLet's try running the sdv as-is on the dataset to see what happens. We'll use\nthe GaussianCopula model on our dataset.\n\nfrom sdv.tabular import GaussianCopula\n\nnp.random.seed(0)\n\nmodel = GaussianCopula(\n  categorical_transformer='label_encoding' # optimize speed\n) \nmodel.fit(data)\n\nNow, let's generate some rows to inspect the synthetic data.\n\nnp.random.seed(12)\nmodel.sample(5)\n\nAlthough the sdv is generating known city names, countries and districts, their\ncombinations don't make sense. We can also go back to our original example and\ngenerate only some rows for Cambridge.\n\nnp.random.seed(10)\n\nconditions = {'Name': 'Cambridge'}\nmodel.sample(5, conditions=conditions)\n\nThe result is a variety of Cambridges that aren't necessarily in USA, GBR, or\nCAN. These aren't valid cities!\n\nWhat's going on? The SDV models include probabilities that some unseen\ncombinations are possible. This is by design: Synthesizing new combinations --\nthat don't blatantly match the original data -- helps with privacy.\n\nHowever in this particular case, we aren't worried about the privacy of a city\nbelonging to a country or district. We actually do want the data to match. This\nis why we need to build a constraint.\n\nFixing the data using rejecting sampling\nIn our previous article [https://sdv.dev/blog/eng-sdv-constraints/], we\ndescribed a solution called reject_sampling that works on any type of constraint\nand is very easy to build: We simply create the synthetic data as usual and then\nthrow out (reject) any data that doesn't match.\n\nIn theory, this can solve our UniqueCombinations constraint. In practice, this\nstrategy is only efficient if the model can easily generate acceptable data.\nLet's calculate the chances of getting an acceptable combination (Name, \nCountryCode, District) from the model.\n\nnp.random.seed(0)\n\n# Sample data from the model\n# The sample may include combinations that aren't valid\nn = 100000\nnew_data = model.sample(n)\n\n# Calculate how many rows are valid\ncombo = ['Name', 'CountryCode', 'District']\nmerged = new_data.merge(data, left_on=combo, right_on=combo, how='left')\npassed = merged[merged['ID_y'].notna()].shape[0]\n\n# Print out our results\nprint(\"Valid rows: \", (passed/n)*100, \"%\")\nprint(\"Rejected rows: \", (1 - passed/n)*100, \"%\")\n\nValid rows:  0.038 %\nRejected rows:  99.96199999999999 %\n\nWith such a low probability of passing the constraint, this strategy can become\nintractable.\n\nFixing the data using transformations\nA more efficient strategy is for the ML model to learn the constraint directly,\nso it always produces acceptable data. We can do this by transforming the data\nin a clever way, forcing the model to learn the logic.\n\nOur previous article [https://sdv.dev/blog/eng-sdv-constraints/] described how\nto do this for a different constraint. Unfortunately, the exact same\ntransformation won't work to solve our current UniqueCombinations constraint. \nThe transform strategy requires a different, creative solution for each\nconstraint. So we have to start from scratch.\n\nCan you think of any other ways to enforce UniqueCombinations?\n\nA solution: Concatenating the data\n\nOne solution is to concatenate the data. That is, rather than treating the city \nName, CountryCode and District as separate items, we treat them as a single\nvalue. This will force the model to learn them as 1 single concept rather than\nas multiple columns that can be recombined.\n\nLet's see this in action.\n\n# create transformed data that concatenates the columns\ndata_transform = data.copy()\n\n# Concatenate the data using a separator\ndata_transform['concatenated'] = data_transform['Name'] + '#' + data_transform['CountryCode'] + '#' + data_transform['District']\n\n# We can drop the individual columns\ndata_transform.drop(labels=['Name', 'CountryCode', 'District'],\n                    axis=1, inplace=True)\n\ndata_transform.head()\n\nNow, we can train the model using the transformed (concatenated) data instead.\n\nnp.random.seed(35)\n\n# create a new model that will learn from the transformed data\nmodel_transform = GaussianCopula(categorical_transformer='label_encoding')\nmodel_transform.fit(data_transform)\n\n# this will produce transformed data\noutput = model_transform.sample()\noutput.head(5)\n\nTo get back realistic-looking data, we can convert the concatenated column back\ninto Name, City and District.\n\nimport pandas as pd\n\n# Split the conatenated column by the separator and save the reuslts\nnames = []\ncountrycodes = []\ndistricts = []\n\nfor x in output['concatenated']:\n  try:\n    name, countrycode, district = x.split('#')\n  except:\n    name, countrycode, district = [np.nan]*3\n  names.append(name)\n  countrycodes.append(countrycode)\n  districts.append(district)\n\n# Add the individual columns back in\noutput['Name'] = pd.Series(names)\noutput['CountryCode'] = pd.Series(countrycodes)\noutput['District'] = pd.Series(districts)\n\n# Drop the concatenated column\noutput.drop(labels=['concatenated'], axis=1, inplace=True)\n\nAs a result, the output now looks like our original data.\n\noutput.head()\n\nMost importantly, the Name, CountryCode and District columns now make sense!\n\nCaveats of transforming the data\n\nThe transform strategy is an efficient and elegant approach to modeling. But\nthere is a downside: The transform strategy might lose some mathematical\nproperties.\n\nTo see why, consider the model's perspective:\n\n * Cambridge#GBR#England is completely different from\n * Cambridge#USA#Massachusetts is completely different from\n * Boston#USA#Massachusetts\n\nThe problem is that two of these actually have something in common -- they are\nlocated in Massachusetts, USA. So the model will not be able to learn anything\nspecial about Massachusetts or USA as a whole.\n\nAs an example, let's see how well the model was able to learn populations of\nUS-based cities.\n\nimport matplotlib.pyplot as plt\n\n# Populations of real US cities\nreal_usa = data.loc[data['CountryCode'] == 'USA', 'Population']\n\n# Populations of synthetic US cities\nsynth_usa = output.loc[output['CountryCode'] == 'USA', 'Population']\n\n# Plot the distributions\nplt.ylabel('US City Data')\nplt.xlabel('Population')\n_ = plt.boxplot([real_usa, synth_usa],\n                showfliers=False,\n                labels=['Real', 'Synthetic'],\n                vert=False\n)\nplt.show()\n\nThe real data shows less variation in city population than the synthetic data.\nThe differences make sense because our model wasn't able to learn about the USA\nas one complete concept.\n\nCan we fix this? It's challenging to fix this issue without degrading the\nmathematical correlations in some other way. If you have any ideas, we welcome\nyou to join our discussion [https://github.com/sdv-dev/SDV/issues/414]!\n\nInputting a UniqueCombination into the SDV\nWe built the constraint -- both the reject_sampling and transform approaches --\ndirectly into the SDV library. If you have sdv installed, this is ready to use.\nImport the UniqueCombinations class from the constraints module.\n\nfrom sdv.constraints import UniqueCombinations\n\n# Create a Unique Combinations constraint\nunique_city_country_district = UniqueCombinations(\n  columns=['Name', 'CountryCode', 'District'],\n  handling_strategy='transform' # you can change this 'reject_sampling' too\n)\n\n# Create a new model using the constraint\nupdated_model = GaussianCopula(\n  constraints=[unique_city_country_district],\n  categorical_transformer='label_encoding'\n)\n\nNow, you can train the model on your data and sample synthetic data.\n\nnp.random.seed(35)\n\nupdated_model.fit(data)\nupdated_model.sample(5)\n\nAll of the synthetic data is guaranteed to follow the UniqueCombinations \nconstraint.\n\nTakeaways\n 1. We can identify a UniqueCombinations requirement by asking: Should it be\n    possible to further mix-and-match the data?\n 2. We can enforce any logical constraint by using reject sampling, which throws\n    out any invalid data. This is not efficient for UniqueCombinations.\n 3. An alternative approach is to transform the data, forcing the ML model to\n    learn the constraint. For UniqueCombinations we transformed the data by\n    concatenating it.\n 4. The logic for UniqueCombinations is already built into the SDV's constraints \n    module, and is ready to use.\n\nFurther reading:\n\n * Engineering Constraints Blog Article\n   [https://sdv.dev/blog/eng-sdv-constraints/]\n * Handling Constraints User Guide\n   [https://sdv.dev/SDV/user_guides/single_table/constraints.html]\n * Tabular Constraints API\n   [https://sdv.dev/SDV/api_reference/constraints/tabular.html]","html":"<p>By default, a machine learning model (ML) may not always learn the deterministic rules in your dataset. We've previously explored how the SDV allows user to <a href=\"https://sdv.dev/blog/eng-sdv-constraints/\" rel=\"nofollow\">input their logic</a> using constraints. With constraints, an SDV model produces logically correct data 100% of the time.</p><p>While an end user might expect the constraint to \"just work,\" engineering this functionality requires some creative techniques. In this article, we'll describe the techniques we used to build the <code>UniqueCombinations</code> constraint. You can also follow along in our <a href=\"https://colab.research.google.com/drive/1bY8y6m7-CjTxWDepw32-ZT3Ubb9RGK5F?usp=sharing\">notebook</a>.</p><pre><code>!pip install sdv==0.13.1</code></pre><pre><code class=\"language-python\">import numpy as np\nimport warnings\n\nwarnings.filterwarnings('ignore')</code></pre><h3 id=\"what-is-a-unique-combinations-constraint\">What is a Unique Combinations Constraint?</h3><p>Users frequently encounter logical constraints on the permutations -- mixing &amp; matching -- that are allowed in synthetic data.</p><p>To illustrate this, let's use the <code>world_v1</code> dataset from the SDV tabular dataset demos. This simple dataset describes the population of different cities around the world.</p><pre><code class=\"language-python\">from sdv.demo import load_tabular_demo\n\ndata = load_tabular_demo('world_v1')\ndata = data.drop(['add_numerical'], axis=1) # not needed for this demo\ndata.head()</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.51.49-AM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1014\" height=\"362\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/01/Screen-Shot-2022-01-19-at-11.51.49-AM.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/01/Screen-Shot-2022-01-19-at-11.51.49-AM.png 1000w, https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.51.49-AM.png 1014w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>Relationship between <code>Name</code>, <code>CountryCode</code> and <code>District</code></strong></p><p>Looking at the data, we can observe that there is a special relationship between the <code>Name</code> of the city, its <code>CountryCode</code> and its geographical <code>District</code>: When generating synthetic data, the model should not blindly mix-and-match these values. Instead, it should <strong>reference the real data to verify whether the combination is valid.</strong> This is called a <code>UniqueCombinations</code> constraint.</p><p>For example, take a particular city, like <code>Cambridge</code>, which appears 3 times in our dataset.</p><pre><code class=\"language-python\">data[data.Name == 'Cambridge']</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.53.07-AM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1020\" height=\"248\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/01/Screen-Shot-2022-01-19-at-11.53.07-AM.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/01/Screen-Shot-2022-01-19-at-11.53.07-AM.png 1000w, https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.53.07-AM.png 1020w\" sizes=\"(min-width: 720px) 720px\"></figure><p>The constraint states that <code>Cambridge</code> should only ever appear with <code>GBR (England)</code>, <code>CAN (Ontario)</code> or <code>USA (Massachusetts)</code>. It is invalid if it appears in any other region -- for eg. Cambridge, France.</p><p><strong>How does the SDV handle a Unique Combination out-of-the-box?</strong></p><p>Let's try running the <code>sdv</code> as-is on the dataset to see what happens. We'll use the <code>GaussianCopula</code> model on our dataset.</p><pre><code class=\"language-python\">from sdv.tabular import GaussianCopula\n\nnp.random.seed(0)\n\nmodel = GaussianCopula(\n  categorical_transformer='label_encoding' # optimize speed\n) \nmodel.fit(data)</code></pre><p>Now, let's generate some rows to inspect the synthetic data.</p><pre><code class=\"language-python\">np.random.seed(12)\nmodel.sample(5)</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.54.31-AM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"940\" height=\"360\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/01/Screen-Shot-2022-01-19-at-11.54.31-AM.png 600w, https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.54.31-AM.png 940w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Although the <code>sdv</code> is generating known city names, countries and districts, their combinations don't make sense. We can also go back to our original example and generate only some rows for <code>Cambridge</code>.</p><pre><code class=\"language-python\">np.random.seed(10)\n\nconditions = {'Name': 'Cambridge'}\nmodel.sample(5, conditions=conditions)</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.55.06-AM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1022\" height=\"364\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/01/Screen-Shot-2022-01-19-at-11.55.06-AM.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/01/Screen-Shot-2022-01-19-at-11.55.06-AM.png 1000w, https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.55.06-AM.png 1022w\" sizes=\"(min-width: 720px) 720px\"></figure><p>The result is a variety of Cambridges that aren't necessarily in USA, GBR, or CAN. These aren't valid cities!</p><p><strong>What's going on?</strong> The SDV models include probabilities that some unseen combinations are possible. This is by design: Synthesizing new combinations -- that don't blatantly match the original data -- helps with privacy.</p><p>However in this particular case, we aren't worried about the privacy of a city belonging to a country or district. We actually <em>do</em> want the data to match. This is why we need to build a constraint.</p><h3 id=\"fixing-the-data-using-rejecting-sampling\">Fixing the data using rejecting sampling</h3><p>In our <a href=\"https://sdv.dev/blog/eng-sdv-constraints/\" rel=\"nofollow\">previous article</a>, we described a solution called <code>reject_sampling</code> that works on any type of constraint and is very easy to build: We simply create the synthetic data as usual and then throw out (reject) any data that doesn't match.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2022/01/UniqueCombinations-02.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"883\" height=\"316\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/01/UniqueCombinations-02.png 600w, https://sdv.ghost.io/content/images/2022/01/UniqueCombinations-02.png 883w\" sizes=\"(min-width: 720px) 720px\"></figure><p>In theory, this can solve our <code>UniqueCombinations</code> constraint. In practice, this strategy is only efficient if the model can easily generate acceptable data. Let's calculate the chances of getting an acceptable combination (<code>Name</code>, <code>CountryCode</code>, <code>District</code>) from the model.</p><pre><code class=\"language-python\">np.random.seed(0)\n\n# Sample data from the model\n# The sample may include combinations that aren't valid\nn = 100000\nnew_data = model.sample(n)\n\n# Calculate how many rows are valid\ncombo = ['Name', 'CountryCode', 'District']\nmerged = new_data.merge(data, left_on=combo, right_on=combo, how='left')\npassed = merged[merged['ID_y'].notna()].shape[0]\n\n# Print out our results\nprint(\"Valid rows: \", (passed/n)*100, \"%\")\nprint(\"Rejected rows: \", (1 - passed/n)*100, \"%\")</code></pre><pre><code>Valid rows:  0.038 %\nRejected rows:  99.96199999999999 %</code></pre><p>With such a low probability of passing the constraint, this strategy can become intractable.</p><h3 id=\"fixing-the-data-using-transformations\">Fixing the data using transformations</h3><p>A more efficient strategy is for the ML model to learn the constraint directly, so it always produces acceptable data. We can do this by transforming the data in a clever way, forcing the model to learn the logic.</p><p>Our <a href=\"https://sdv.dev/blog/eng-sdv-constraints/\" rel=\"nofollow\">previous article</a> described how to do this for a different constraint. Unfortunately, the exact same transformation won't work to solve our current <code>UniqueCombinations</code> constraint. <strong>The transform strategy requires a different, creative solution for each constraint.</strong> So we have to start from scratch.</p><p>Can you think of any other ways to enforce <code>UniqueCombinations</code>?</p><p><strong>A solution: Concatenating the data</strong></p><p>One solution is to concatenate the data. That is, rather than treating the city <code>Name</code>, <code>CountryCode</code> and <code>District</code> as separate items, we treat them as a single value. This will force the model to learn them as 1 single concept rather than as multiple columns that can be recombined.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2022/01/UniqueCombinations-01.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1524\" height=\"1200\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/01/UniqueCombinations-01.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/01/UniqueCombinations-01.png 1000w, https://sdv.ghost.io/content/images/2022/01/UniqueCombinations-01.png 1524w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Let's see this in action.</p><pre><code class=\"language-python\"># create transformed data that concatenates the columns\ndata_transform = data.copy()\n\n# Concatenate the data using a separator\ndata_transform['concatenated'] = data_transform['Name'] + '#' + data_transform['CountryCode'] + '#' + data_transform['District']\n\n# We can drop the individual columns\ndata_transform.drop(labels=['Name', 'CountryCode', 'District'],\n                    axis=1, inplace=True)\n\ndata_transform.head()</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.58.21-AM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"828\" height=\"368\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/01/Screen-Shot-2022-01-19-at-11.58.21-AM.png 600w, https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.58.21-AM.png 828w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Now, we can train the model using the transformed (concatenated) data instead.</p><pre><code class=\"language-python\">np.random.seed(35)\n\n# create a new model that will learn from the transformed data\nmodel_transform = GaussianCopula(categorical_transformer='label_encoding')\nmodel_transform.fit(data_transform)\n\n# this will produce transformed data\noutput = model_transform.sample()\noutput.head(5)</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.58.53-AM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"882\" height=\"368\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/01/Screen-Shot-2022-01-19-at-11.58.53-AM.png 600w, https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.58.53-AM.png 882w\" sizes=\"(min-width: 720px) 720px\"></figure><p>To get back realistic-looking data, we can convert the concatenated column back into <code>Name</code>, <code>City</code> and <code>District</code>.</p><pre><code class=\"language-python\">import pandas as pd\n\n# Split the conatenated column by the separator and save the reuslts\nnames = []\ncountrycodes = []\ndistricts = []\n\nfor x in output['concatenated']:\n  try:\n    name, countrycode, district = x.split('#')\n  except:\n    name, countrycode, district = [np.nan]*3\n  names.append(name)\n  countrycodes.append(countrycode)\n  districts.append(district)\n\n# Add the individual columns back in\noutput['Name'] = pd.Series(names)\noutput['CountryCode'] = pd.Series(countrycodes)\noutput['District'] = pd.Series(districts)\n\n# Drop the concatenated column\noutput.drop(labels=['concatenated'], axis=1, inplace=True)</code></pre><p>As a result, the output now looks like our original data.</p><pre><code class=\"language-python\">output.head()</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.59.41-AM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1020\" height=\"368\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/01/Screen-Shot-2022-01-19-at-11.59.41-AM.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/01/Screen-Shot-2022-01-19-at-11.59.41-AM.png 1000w, https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-11.59.41-AM.png 1020w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Most importantly, the <code>Name</code>, <code>CountryCode</code> and <code>District</code> columns now make sense!</p><p><strong>Caveats of transforming the data</strong></p><p>The transform strategy is an efficient and elegant approach to modeling. But there is a downside: <strong>The transform strategy might lose some mathematical properties.</strong></p><p>To see why, consider the model's perspective:</p><ul><li><code>Cambridge#GBR#England</code> is completely different from</li><li><code>Cambridge#USA#Massachusetts</code> is completely different from</li><li><code>Boston#USA#Massachusetts</code></li></ul><p>The problem is that two of these actually have something in common -- they are located in <code>Massachusetts, USA</code>. So the model will not be able to learn anything special about <code>Massachusetts</code> or <code>USA</code> as a whole.</p><p>As an example, let's see how well the model was able to learn populations of US-based cities.</p><pre><code class=\"language-python\">import matplotlib.pyplot as plt\n\n# Populations of real US cities\nreal_usa = data.loc[data['CountryCode'] == 'USA', 'Population']\n\n# Populations of synthetic US cities\nsynth_usa = output.loc[output['CountryCode'] == 'USA', 'Population']\n\n# Plot the distributions\nplt.ylabel('US City Data')\nplt.xlabel('Population')\n_ = plt.boxplot([real_usa, synth_usa],\n                showfliers=False,\n                labels=['Real', 'Synthetic'],\n                vert=False\n)\nplt.show()</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-12.00.53-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1022\" height=\"500\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/01/Screen-Shot-2022-01-19-at-12.00.53-PM.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/01/Screen-Shot-2022-01-19-at-12.00.53-PM.png 1000w, https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-12.00.53-PM.png 1022w\" sizes=\"(min-width: 720px) 720px\"></figure><p>The real data shows less variation in city population than the synthetic data. The differences make sense because our model wasn't able to learn about the USA as one complete concept.</p><p><strong>Can we fix this?</strong> It's challenging to fix this issue without degrading the mathematical correlations in some other way. If you have any ideas, we welcome you to <a href=\"https://github.com/sdv-dev/SDV/issues/414\" rel=\"nofollow\">join our discussion</a>!</p><h3 id=\"inputting-a-uniquecombination-into-the-sdv\">Inputting a UniqueCombination into the SDV</h3><p>We built the constraint -- both the <code>reject_sampling</code> and <code>transform</code> approaches -- directly into the SDV library. If you have <code>sdv</code> installed, this is ready to use. Import the <code>UniqueCombinations</code> class from the <code>constraints</code> module.</p><pre><code class=\"language-python\">from sdv.constraints import UniqueCombinations\n\n# Create a Unique Combinations constraint\nunique_city_country_district = UniqueCombinations(\n  columns=['Name', 'CountryCode', 'District'],\n  handling_strategy='transform' # you can change this 'reject_sampling' too\n)\n\n# Create a new model using the constraint\nupdated_model = GaussianCopula(\n  constraints=[unique_city_country_district],\n  categorical_transformer='label_encoding'\n)</code></pre><p>Now, you can train the model on your data and sample synthetic data.</p><pre><code class=\"language-python\">np.random.seed(35)\n\nupdated_model.fit(data)\nupdated_model.sample(5)</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-12.02.30-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1146\" height=\"382\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/01/Screen-Shot-2022-01-19-at-12.02.30-PM.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/01/Screen-Shot-2022-01-19-at-12.02.30-PM.png 1000w, https://sdv.ghost.io/content/images/2022/01/Screen-Shot-2022-01-19-at-12.02.30-PM.png 1146w\" sizes=\"(min-width: 720px) 720px\"></figure><p>All of the synthetic data is guaranteed to follow the <code>UniqueCombinations</code> constraint.</p><h3 id=\"takeaways\">Takeaways</h3><ol><li>We can identify a <code>UniqueCombinations</code> requirement by asking: Should it be possible to further mix-and-match the data?</li><li>We can enforce any logical constraint by using reject sampling, which throws out any invalid data. This is not efficient for <code>UniqueCombinations</code>.</li><li>An alternative approach is to transform the data, forcing the ML model to learn the constraint. For <code>UniqueCombinations</code> we transformed the data by concatenating it.</li><li>The logic for <code>UniqueCombinations</code> is already built into the SDV's <code>constraints</code> module, and is ready to use.</li></ol><p>Further reading:</p><ul><li><a href=\"https://sdv.dev/blog/eng-sdv-constraints/\" rel=\"nofollow\">Engineering Constraints Blog Article</a></li><li><a href=\"https://sdv.dev/SDV/user_guides/single_table/constraints.html\" rel=\"nofollow\">Handling Constraints User Guide</a></li><li><a href=\"https://sdv.dev/SDV/api_reference/constraints/tabular.html\" rel=\"nofollow\">Tabular Constraints API</a></li></ul>","url":"https://sdv.ghost.io/building-unique-combinations/","canonical_url":null,"uuid":"01fb2714-6b15-4055-8a77-e7fde4a0f944","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"61e841116361ff003b9ca712","reading_time":7}},{"node":{"id":"Ghost__Post__61c10f636317ec003be8e39d","title":"How we engineered constraint handling strategies in SDV","slug":"eng-sdv-constraints","featured":false,"feature_image":"https://sdv.ghost.io/content/images/2021/12/Banner-01.png","excerpt":"The SDV enforces deterministic rules using constraints. What strategies did we use to engineer this ML system? Dive into the details.","custom_excerpt":"The SDV enforces deterministic rules using constraints. What strategies did we use to engineer this ML system? Dive into the details.","visibility":"public","created_at_pretty":"20 December, 2021","published_at_pretty":"21 December, 2021","updated_at_pretty":"21 December, 2021","created_at":"2021-12-20T18:18:59.000-05:00","published_at":"2021-12-20T19:14:45.000-05:00","updated_at":"2021-12-21T14:23:08.000-05:00","meta_title":"How we engineered constraint handling strategies in the SDV","meta_description":"The SDV enforces deterministic rules using constraints. What strategies did we use to engineer this ML system? Dive into the details.","og_description":null,"og_image":null,"og_title":null,"twitter_description":"The SDV enforces deterministic rules using constraints. What strategies did we use to engineer this ML system?","twitter_image":null,"twitter_title":"How we engineered constraint handling strategies in SDV","authors":[{"name":"Andrew Montanez","slug":"andrew","bio":"For his Master's thesis at MIT, Andrew developed and open sourced the SDV libraries. He wants to use his knowledge of SDV and engineering experience to build a leading product in synthetic data.","profile_image":"https://sdv.ghost.io/content/images/2021/12/Andrew_Montanez.jpg","twitter":null,"facebook":null,"website":"https://www.linkedin.com/in/andrew-montanez-593247b2/"}],"primary_author":{"name":"Andrew Montanez","slug":"andrew","bio":"For his Master's thesis at MIT, Andrew developed and open sourced the SDV libraries. He wants to use his knowledge of SDV and engineering experience to build a leading product in synthetic data.","profile_image":"https://sdv.ghost.io/content/images/2021/12/Andrew_Montanez.jpg","twitter":null,"facebook":null,"website":"https://www.linkedin.com/in/andrew-montanez-593247b2/"},"primary_tag":{"name":"Engineering","slug":"engineering","description":"The SDV engineering team is serving a global, open source user base. In our engineering blog, we highlight engineering challenges and design decisions we've made in support of our community.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Engineering","slug":"engineering","description":"The SDV engineering team is serving a global, open source user base. In our engineering blog, we highlight engineering challenges and design decisions we've made in support of our community.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"The SDV uses machine learning (ML) to automatically learn rules (aka\ncorrelations) from real data and generate accurate synthetic data. While these\nmodels are powerful, they may not learn everything. In our previous article\n[https://sdv.dev/blog/user-input-synthetic-data/], we described how the SDV\nmodels may not learn deterministic rules. These are patterns and laws that are\ninherent to the dataset:\n\n * They are unchangeable, no matter what data you input.\n * They describe rules that must apply to every row, no exceptions.\n\nLuckily, it's possible for you to improve the machine learning model: When you\ninput constraints, it ensures the model will learn deterministic rules and\nultimately improve the quality of your synthetic data.\n\nIn this article, we'll dive into the technical details of how you can apply\nconstraints and how they work under-the-hood. You can also follow along in our \nnotebook\n[https://colab.research.google.com/drive/1cVGv2Xtzhd9qHgbkjsYLeLzsA8bDd1uA?usp=sharing]\n.\n\n!pip install sdv==0.13.0\n\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nThe Dataset\nThe dataset we're using comes from a Kaggle Competition\n[https://www.kaggle.com/c/expedia-hotel-recommendations/data?select=train.csv] \nhosted by Expedia. We've modified the data slightly for our use.\n\nfrom sdv.demo import load_tabular_demo\n\ndata = load_tabular_demo('expedia_hotel_logs')\n\nIn this real-world dataset, each row represents a search result for a hotel\nbooking.\n\nFor the purposes of this notebook, we'll drop some columns that aren't useful to\nus.\n\nimport pandas as pd\n\n# Drop some columns that aren't useful for this demo\ndrop_columns = ['date_time', 'user_location_country', 'user_location_region',\n                'user_location_city', 'user_id', 'srch_destination_id',\n                'hotel_country', 'hotel_market', 'hotel_cluster',\n                'srch_destination_type_id', 'orig_destination_distance',\n                'posa_continent', 'site_name', 'channel']\ndata = data.drop(drop_columns, axis=1)\n\n# make sure these columns are read as datetimes\nfor col in ['srch_ci', 'srch_co']:\n  data[col] = pd.to_datetime(data[col])\n\n# Inspect the data\ndata.head()\n\nThe search parameters, for finding a hotel room, saved in this dataset come from\nfrom user's input. For example:\n\nDeterministic Rule\n\nIn order for the search to be valid, the searched check-in date must happen\nbefore the searched check-out date. That is: srch_ci < srch_co.\n\nThis is an inherent property of any search, not just for this particular dataset\n-- we call this a deterministic rule. We can verify if this is true by checking\nfor any exceptions.\n\nprint('Violations of the deterministic rule')\nlen(data[data['srch_ci'] > data['srch_co']])\n\n0\n\nWill SDV's machine learning model learn this out of the box?\n\nTo test this, let's use SDV to learn a GaussianCopula model from the data and\nsample synthetic data.\n\nfrom sdv.tabular import GaussianCopula\n\nnp.random.seed(0)\n\nmodel = GaussianCopula(primary_key='log_id')\nmodel.fit(data)\n\nsynth_data = model.sample(500)\nsynth_data.head()\n\nNow, we can inspect the synthetic data to see if there are any invalid rows.\n\ninvalid_row_indices = synth_data['srch_ci'] > synth_data['srch_co']\ninvalid_rows = synth_data[invalid_row_indices]\n\nnum_invalid = len(invalid_rows)\nperc_invalid = num_invalid / len(synth_data) * 100\nprint('Number of invalid rows:', num_invalid, '(', round(perc_invalid, 2), '%)')\n\ninvalid_rows.head()\n\nThe majority of the rows (94.8%) are valid, meaning the model learned the rule\npretty accurately. It learned probabilistically that if the srch_ci is higher \nsrch_co should be even higher. However, some invalid rows (~5%) are still\ncreated so the model did not learn this deterministic rule.\n\nThis raises the question: What can we do to enforce a deterministic rule?\n\nImproving the synthetic data\nLet's explore some options for enforcing our deterministic rule in order to\nimprove the overall quality of the synthetic data.\n\nRejecting invalid data\n\nThe simplest solution is to simply drop the invalid rows, and continually sample\nfrom the model until the desired amount of valid rows are produced. We call this \nreject sampling.\n\nThe code below performs reject sampling until we have synthesized 500 rows.\n\nimport pandas as pd\n\n# Keep track of how many valid rows we've sampled\nnum_valid_rows = synth_data.shape[0] - invalid_rows.shape[0]\n\nwhile num_valid_rows < 500:\n  # Reject the invalid data \n  synth_data = synth_data.drop(invalid_rows.index)\n  \n  # Create new data to replace the invalid data\n  new_data = model.sample(500-num_valid_rows)\n  synth_data = pd.concat([synth_data, new_data])\n  invalid_rows = synth_data[synth_data['srch_ci'] > synth_data['srch_co']]\n  num_valid_rows = synth_data.shape[0] - invalid_rows.shape[0]\n\nsynth_data.reset_index(drop=True, inplace=True)\n\nNow, there are no invalid rows in our dataset.\n\ninvalid_rows = synth_data[synth_data['srch_ci'] > synth_data['srch_co']]\ninvalid_rows.shape[0]\n\n0\n\nIn this example, we got lucky. Only a small percentage of the rows were invalid\neach time sample was called.\n\nWhat would happen if majority of the rows were invalid every time we sampled? It\nwould take a longer time to get all the desired rows. Sampling time is the\nprimary drawback of reject sampling. Is there another approach we can use to\nimprove the time?\n\nTransforming your data\n\nInstead of reject sampling, what if the model never produced invalid rows in the\nfirst place? To achieve this, we can alter the input data to the model so it's\nforced to learn the constraint.\n\nLet's stop giving the srch_ci and srch_co to the model. Instead, let's teach the\nmodel to learn the srch_ci and the difference between the dates.\n\ndifference = srch_co - srch_ci\n\nThe model will produce srch_ci and difference as a result. Then, we can\nre-compute srch_co with the opposite formula.\n\nsrch_co = srch_ci + difference\n\n(Of course, we need to make sure the difference is always positive, which we can\ndo using a log + 1.)\n\nLet's see this in action.\n\n# Compute the difference\ndiff = (data['srch_co'] - data['srch_ci']).astype('timedelta64[D]')\n\n# Take the log and add one to ensure that it's positive\ndate_diff = np.log(diff + 1)\n\n# The model should learn this column instead of the checkout date\nmodified_data = data.drop('srch_co', axis=1)\nmodified_data['difference'] = date_diff\nmodified_data[['srch_ci', 'difference']].head()\n\nNow, we can fit the model with the modified data. The new samples will include\nthe srch_ci and date_diff columns.\n\nnp.random.seed(20)\n\nmodified_model = GaussianCopula(primary_key='log_id')\nmodified_model.fit(modified_data)\n\nmodified_synth_data = modified_model.sample(500)\nmodified_synth_data[['srch_ci', 'difference']].head()\n\nWe can recompute the srch_co based on srch_ci and difference.\n\n# Undo the log+1 that we added\ndiff = (np.exp(modified_synth_data['difference'].values).round() - 1).clip(0).astype('timedelta64[ns]')\n\n# Reconstruct the end_date and remove the date_diff column\nmodified_synth_data['srch_co'] = modified_synth_data['srch_ci'] + diff\nmodified_synth_data = modified_synth_data.drop('difference', axis=1)\n\nmodified_synth_data.head()\n\nLet's verify that this computation does not create any invalid rows.\n\ninvalid_rows = modified_synth_data[modified_synth_data['srch_ci'] > modified_synth_data['srch_co']]\ninvalid_rows.shape[0]\n\n0\n\nThe transformation worked! In our case, this was a more efficient way to enforce\nthe deterministic rule.\n\nBut if our rule were more complex -- and we couldn't think of a transformation\n-- we could always fall back to reject sampling.\n\nInputting deterministic rules in the SDV\nWe've seen how reject sampling and transform can be used to improve the quality\nof the synthetic data by accounting for deterministic rules. However, it may be\ncumbersome for you to manually implement these strategies. In fact, we saw some\ncommon problems in our SDV user community:\n\n * Users had multiple deterministic rules in their dataset. For example, there\n   could be multiple comparisons between different pairs of columns.\n * Users from multiple domains often had the same kind of deterministic rule.\n   For example, one column being greater than another is a common deterministic\n   rule, agonistic of a use case or domain.\n\nTo solve these problems, we introduced a constraints module in the SDV. With the\nconstraints module, SDV users can easily input deterministic rules. Let's look\nat an example.\n\nUsing the SDV constraints module\n\nThe constraints module in the SDV contains several different types of\npre-defined deterministic rules.\n\nWe will use the GreaterThan constraint, which will enforce that one column's\nvalues are always greater than another's.\n\nfrom sdv.constraints import GreaterThan\n\nNext, we can input the logic of our deterministic rule by creating a constraint\nobject. The GreaterThan constraint accepts the column names as input.\n\ngt_constraint = GreaterThan(\n  low='srch_ci',\n  high='srch_co')\n\nFinally, we can input this constraint when instantiating the model.\n\nnp.random.seed(10)\n\n# Apply the constraint to the model\nmodel_with_constraint = GaussianCopula(\n  primary_key='log_id',\n  constraints=[gt_constraint])\n\nmodel_with_constraint.fit(data)\n\n# Sample synthetic data\nconstrained_data = model_with_constraint.sample(500)\nconstrained_data.head()\n\nAs a result, we should see that all 500 generated rows are valid on the first\ntry. No invalid rows are present in our dataset.\n\ninvalid_rows = constrained_data[constrained_data['srch_ci'] > constrained_data['srch_co']]\ninvalid_rows.shape[0]\n\n0\n\nUsing the SDV was much simpler than writing the code ourselves! Plus, we can\ncreate multiple constraints for the same dataset an easily use them on other\ndatasets.\n\nSpecifying the strategy in the constraints module\n\nBy default, the GreaterThan constraint uses the transform strategy. However, you\ncan use the handling_strategy argument to control this. This argument accepts \n'reject_sampling' or 'transform' as valid strategies.\n\ngt_reject_constraint = GreaterThan(\n  low='srch_ci',\n  high='srch_co',\n  handling_strategy='reject_sampling' # specify the strategy\n)\n\nSimilar to before, we can then input this constraint into the model.\n\nnp.random.seed(30)\n\n# Apply the constraint to the model\nmodel_with_reject_constraint = GaussianCopula(\n  primary_key='log_id',\n  constraints=[gt_reject_constraint])\n\nmodel_with_reject_constraint.fit(data)\n\n# Sample synthetic data\nconstrained_reject_data = model_with_reject_constraint.sample(500)\nconstrained_reject_data.head()\n\ninvalid_rows = constrained_reject_data[constrained_reject_data['srch_ci'] > constrained_reject_data['srch_co']\ninvalid_rows.shape[0]\n\n0\n\nWhat other deterministic rules are already available in SDV?\nThe GreaterThan constraint is one kind of deterministic rule, but there may be\nothers that apply to your dataset. The SDV offers more constraints for other\ntypes of logic.\n\n * Unique\n   [https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#unique-constraint] \n   when values in a column must be unique to the entire dataset.\n * UniqueCombinations\n   [https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#uniquecombinations-constraint] \n   to limit the permutations between multiple columns.\n * Positive and Negative\n   [https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#positive-and-negative-constraints] \n   to enforce boundaries.\n * ColumnFormula\n   [https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#columnformula-constraint] \n   when there is a formulaic association between columns.\n * Rounding\n   [https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#rounding-constraint] \n   to enforce decimal precision.\n * Between\n   [https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#between-constraint] \n   when one column's values must be between 2 other values.\n * OneHotEncoding\n   [https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#onehotencoding-constraint] \n   when your data includes a variable with one hot encoding.\n\nFor each of them, you can specify handling strategies for reject_sampling to\ndiscard invalid data or transform to modify the data (unique to each\nconstraint).\n\nWhat if my rule isn't included in the module?\n\nYou may come across a rule that cannot be described by any of the constraints\nclasses in the SDV. In this case, you can define a CustomConstraint\n[https://sdv.dev/SDV/user_guides/single_table/custom_constraints.html#defining-custom-constraints] \nwith logic specific to your use case.\n\nAdditionally, consider filing a feature request on GitHub\n[https://github.com/sdv-dev/SDV/issues/new/choose] with details about your use\ncase & scenario. We can add your logic as a pre-defined constraint so others can\nbenefit from it too!\n\nTakeaways\nIn this notebook, we explored what happens when we have a deterministic rule in\nour dataset.\n\n 1. Machine learning models may not able to learn the deterministic rules out of\n    the box, but it is possible to improve the model to learn these types of\n    rules.\n 2. Deterministic rules can be handled by discarding invalid data (reject\n    sampling) or by adding some clever preprocessing to your code (transforming\n    ).\n 3. The SDV offers a constraints module that allows you to input commonly found\n    deterministic rules. You can specify the handling strategy for each\n    constraint and apply multiple rules to the same dataset.\n\nFurther Reading\n\nFor further information about constraints refer to the Handling Constraints\nUser\nGuide [https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html].","html":"<p>The SDV uses machine learning (ML) to automatically learn rules (aka correlations) from real data and generate accurate synthetic data. While these models are powerful, they may not learn everything. In our <a href=\"https://sdv.dev/blog/user-input-synthetic-data/\" rel=\"nofollow\">previous article</a>, we described how the SDV models may not learn <strong>deterministic rules</strong>. These are patterns and laws that are inherent to the dataset:</p><ul><li>They are unchangeable, no matter what data you input.</li><li>They describe rules that must apply to every row, no exceptions.</li></ul><p>Luckily, it's possible for you to improve the machine learning model: When you input constraints, it ensures the model will learn deterministic rules and ultimately improve the quality of your synthetic data.</p><p>In this article, we'll dive into the technical details of how you can apply constraints and how they work under-the-hood. You can also follow along in our <a href=\"https://colab.research.google.com/drive/1cVGv2Xtzhd9qHgbkjsYLeLzsA8bDd1uA?usp=sharing\">notebook</a>.</p><pre><code>!pip install sdv==0.13.0</code></pre><pre><code class=\"language-python\">import numpy as np\nimport warnings\n\nwarnings.filterwarnings('ignore')</code></pre><h3 id=\"the-dataset\">The Dataset</h3><p>The dataset we're using comes from a <a href=\"https://www.kaggle.com/c/expedia-hotel-recommendations/data?select=train.csv\" rel=\"nofollow\">Kaggle Competition</a> hosted by Expedia. We've modified the data slightly for our use.</p><pre><code class=\"language-python\">from sdv.demo import load_tabular_demo\n\ndata = load_tabular_demo('expedia_hotel_logs')</code></pre><p>In this real-world dataset, each row represents a search result for a hotel booking.</p><p>For the purposes of this notebook, we'll drop some columns that aren't useful to us.</p><pre><code class=\"language-python\">import pandas as pd\n\n# Drop some columns that aren't useful for this demo\ndrop_columns = ['date_time', 'user_location_country', 'user_location_region',\n                'user_location_city', 'user_id', 'srch_destination_id',\n                'hotel_country', 'hotel_market', 'hotel_cluster',\n                'srch_destination_type_id', 'orig_destination_distance',\n                'posa_continent', 'site_name', 'channel']\ndata = data.drop(drop_columns, axis=1)\n\n# make sure these columns are read as datetimes\nfor col in ['srch_ci', 'srch_co']:\n  data[col] = pd.to_datetime(data[col])\n\n# Inspect the data\ndata.head()</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.36.14-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"349\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/12/Screen-Shot-2021-12-20-at-3.36.14-PM.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/12/Screen-Shot-2021-12-20-at-3.36.14-PM.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/12/Screen-Shot-2021-12-20-at-3.36.14-PM.png 1600w, https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.36.14-PM.png 2122w\" sizes=\"(min-width: 720px) 720px\"></figure><p>The search parameters, for finding a hotel room, saved in this dataset come from from user's input. For example:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2021/12/EngineeredConstraint-08.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"912\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/12/EngineeredConstraint-08.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/12/EngineeredConstraint-08.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/12/EngineeredConstraint-08.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2021/12/EngineeredConstraint-08.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>Deterministic Rule</strong></p><p>In order for the search to be valid, the searched check-in date must happen before the searched check-out date. That is: <code>srch_ci &lt; srch_co</code>.</p><p>This is an inherent property of any search, not just for this particular dataset -- we call this a <strong>deterministic rule</strong>. We can verify if this is true by checking for any exceptions.</p><pre><code class=\"language-python\">print('Violations of the deterministic rule')\nlen(data[data['srch_ci'] &gt; data['srch_co']])</code></pre><pre><code>0</code></pre><p><strong>Will SDV's machine learning model learn this out of the box?</strong></p><p>To test this, let's use SDV to learn a <code>GaussianCopula</code> model from the data and sample synthetic data.</p><pre><code class=\"language-python\">from sdv.tabular import GaussianCopula\n\nnp.random.seed(0)\n\nmodel = GaussianCopula(primary_key='log_id')\nmodel.fit(data)\n\nsynth_data = model.sample(500)\nsynth_data.head()</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.36.54-PM-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"388\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/12/Screen-Shot-2021-12-20-at-3.36.54-PM-1.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/12/Screen-Shot-2021-12-20-at-3.36.54-PM-1.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/12/Screen-Shot-2021-12-20-at-3.36.54-PM-1.png 1600w, https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.36.54-PM-1.png 2050w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Now, we can inspect the synthetic data to see if there are any invalid rows.</p><pre><code class=\"language-python\">invalid_row_indices = synth_data['srch_ci'] &gt; synth_data['srch_co']\ninvalid_rows = synth_data[invalid_row_indices]\n\nnum_invalid = len(invalid_rows)\nperc_invalid = num_invalid / len(synth_data) * 100\nprint('Number of invalid rows:', num_invalid, '(', round(perc_invalid, 2), '%)')\n\ninvalid_rows.head()</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.37.27-PM-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"414\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/12/Screen-Shot-2021-12-20-at-3.37.27-PM-1.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/12/Screen-Shot-2021-12-20-at-3.37.27-PM-1.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/12/Screen-Shot-2021-12-20-at-3.37.27-PM-1.png 1600w, https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.37.27-PM-1.png 2070w\" sizes=\"(min-width: 720px) 720px\"></figure><p>The majority of the rows (94.8%) are valid, meaning the model learned the rule pretty accurately. It learned probabilistically that if the <code>srch_ci</code> is higher <code>srch_co</code> should be even higher. However, some invalid rows (~5%) are still created so <strong>the model did not learn this deterministic rule.</strong></p><p>This raises the question: What can we do to enforce a deterministic rule?</p><h3 id=\"improving-the-synthetic-data\">Improving the synthetic data</h3><p>Let's explore some options for enforcing our deterministic rule in order to improve the overall quality of the synthetic data.</p><p><strong>Rejecting invalid data</strong></p><p>The simplest solution is to simply drop the invalid rows, and continually sample from the model until the desired amount of valid rows are produced. We call this <strong>reject sampling</strong>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2021/12/EngineeredConstraint-07--1-.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"493\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/12/EngineeredConstraint-07--1-.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/12/EngineeredConstraint-07--1-.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/12/EngineeredConstraint-07--1-.png 1600w, https://sdv.ghost.io/content/images/2021/12/EngineeredConstraint-07--1-.png 2071w\" sizes=\"(min-width: 720px) 720px\"></figure><p>The code below performs reject sampling until we have synthesized 500 rows.</p><pre><code class=\"language-python\">import pandas as pd\n\n# Keep track of how many valid rows we've sampled\nnum_valid_rows = synth_data.shape[0] - invalid_rows.shape[0]\n\nwhile num_valid_rows &lt; 500:\n  # Reject the invalid data \n  synth_data = synth_data.drop(invalid_rows.index)\n  \n  # Create new data to replace the invalid data\n  new_data = model.sample(500-num_valid_rows)\n  synth_data = pd.concat([synth_data, new_data])\n  invalid_rows = synth_data[synth_data['srch_ci'] &gt; synth_data['srch_co']]\n  num_valid_rows = synth_data.shape[0] - invalid_rows.shape[0]\n\nsynth_data.reset_index(drop=True, inplace=True)</code></pre><p>Now, there are no invalid rows in our dataset.</p><pre><code class=\"language-python\">invalid_rows = synth_data[synth_data['srch_ci'] &gt; synth_data['srch_co']]\ninvalid_rows.shape[0]</code></pre><pre><code>0</code></pre><p>In this example, we got lucky. Only a small percentage of the rows were invalid each time <code>sample</code> was called.</p><p>What would happen if majority of the rows were invalid every time we sampled? It would take a longer time to get all the desired rows. <strong>Sampling time is the primary drawback of reject sampling. </strong>Is there another approach we can use to improve the time?</p><p><strong>Transforming your data</strong></p><p>Instead of reject sampling, what if the model never produced invalid rows in the first place? To achieve this, we can alter the input data to the model so it's forced to learn the constraint.</p><p>Let's stop giving the <code>srch_ci</code> and <code>srch_co</code> to the model. Instead, let's teach the model to learn the <code>srch_ci</code> and the <code>difference</code> between the dates.</p><pre><code>difference = srch_co - srch_ci</code></pre><p>The model will produce <code>srch_ci</code> and <code>difference</code> as a result. Then, we can re-compute <code>srch_co</code> with the opposite formula.</p><pre><code>srch_co = srch_ci + difference</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2021/12/EngineeredConstraint-06--1-.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"879\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/12/EngineeredConstraint-06--1-.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/12/EngineeredConstraint-06--1-.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/12/EngineeredConstraint-06--1-.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2021/12/EngineeredConstraint-06--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>(Of course, we need to make sure the difference is always positive, which we can do using a <code>log + 1</code>.)</p><p>Let's see this in action.</p><pre><code class=\"language-python\"># Compute the difference\ndiff = (data['srch_co'] - data['srch_ci']).astype('timedelta64[D]')\n\n# Take the log and add one to ensure that it's positive\ndate_diff = np.log(diff + 1)\n\n# The model should learn this column instead of the checkout date\nmodified_data = data.drop('srch_co', axis=1)\nmodified_data['difference'] = date_diff\nmodified_data[['srch_ci', 'difference']].head()</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.30.15-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"390\" height=\"360\"></figure><p>Now, we can fit the model with the modified data. The new samples will include the <code>srch_ci</code> and <code>date_diff</code> columns.</p><pre><code class=\"language-python\">np.random.seed(20)\n\nmodified_model = GaussianCopula(primary_key='log_id')\nmodified_model.fit(modified_data)\n\nmodified_synth_data = modified_model.sample(500)\nmodified_synth_data[['srch_ci', 'difference']].head()</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.31.03-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"392\" height=\"356\"></figure><p>We can recompute the <code>srch_co</code> based on <code>srch_ci</code> and <code>difference</code>.</p><pre><code class=\"language-python\"># Undo the log+1 that we added\ndiff = (np.exp(modified_synth_data['difference'].values).round() - 1).clip(0).astype('timedelta64[ns]')\n\n# Reconstruct the end_date and remove the date_diff column\nmodified_synth_data['srch_co'] = modified_synth_data['srch_ci'] + diff\nmodified_synth_data = modified_synth_data.drop('difference', axis=1)\n\nmodified_synth_data.head()</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.38.38-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"491\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/12/Screen-Shot-2021-12-20-at-3.38.38-PM.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/12/Screen-Shot-2021-12-20-at-3.38.38-PM.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/12/Screen-Shot-2021-12-20-at-3.38.38-PM.png 1600w, https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.38.38-PM.png 2142w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Let's verify that this computation does not create any invalid rows.</p><pre><code class=\"language-python\">invalid_rows = modified_synth_data[modified_synth_data['srch_ci'] &gt; modified_synth_data['srch_co']]\ninvalid_rows.shape[0]</code></pre><pre><code>0</code></pre><p>The transformation worked! In our case, this was a more efficient way to enforce the deterministic rule.</p><p>But if our rule were more complex -- and we couldn't think of a transformation -- we could always fall back to reject sampling.</p><h3 id=\"inputting-deterministic-rules-in-the-sdv\">Inputting deterministic rules in the SDV</h3><p>We've seen how reject sampling and transform can be used to improve the quality of the synthetic data by accounting for deterministic rules. However, it may be cumbersome for you to manually implement these strategies. In fact, we saw some common problems in our SDV user community:</p><ul><li>Users had multiple deterministic rules in their dataset. For example, there could be multiple comparisons between different pairs of columns.</li><li>Users from multiple domains often had the same kind of deterministic rule. For example, one column being greater than another is a common deterministic rule, agonistic of a use case or domain.</li></ul><p>To solve these problems, we introduced a constraints module in the SDV. <strong>With the constraints module, SDV users can easily input deterministic rules. </strong>Let's look at an example.</p><p><strong>Using the SDV constraints module</strong></p><p>The <code>constraints</code> module in the SDV contains several different types of pre-defined deterministic rules.</p><p>We will use the <code>GreaterThan</code> constraint, which will enforce that one column's values are always greater than another's.</p><pre><code class=\"language-python\">from sdv.constraints import GreaterThan</code></pre><p>Next, we can input the logic of our deterministic rule by creating a constraint object. The <code>GreaterThan</code> constraint accepts the column names as input.</p><pre><code class=\"language-python\">gt_constraint = GreaterThan(\n  low='srch_ci',\n  high='srch_co')</code></pre><p>Finally, we can input this constraint when instantiating the model.</p><pre><code class=\"language-python\">np.random.seed(10)\n\n# Apply the constraint to the model\nmodel_with_constraint = GaussianCopula(\n  primary_key='log_id',\n  constraints=[gt_constraint])\n\nmodel_with_constraint.fit(data)\n\n# Sample synthetic data\nconstrained_data = model_with_constraint.sample(500)\nconstrained_data.head()</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.39.11-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"389\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/12/Screen-Shot-2021-12-20-at-3.39.11-PM.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/12/Screen-Shot-2021-12-20-at-3.39.11-PM.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/12/Screen-Shot-2021-12-20-at-3.39.11-PM.png 1600w, https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.39.11-PM.png 2046w\" sizes=\"(min-width: 720px) 720px\"></figure><p>As a result, we should see that all 500 generated rows are valid on the first try. No invalid rows are present in our dataset.</p><pre><code class=\"language-python\">invalid_rows = constrained_data[constrained_data['srch_ci'] &gt; constrained_data['srch_co']]\ninvalid_rows.shape[0]</code></pre><pre><code>0</code></pre><p>Using the SDV was much simpler than writing the code ourselves! Plus, we can create multiple constraints for the same dataset an easily use them on other datasets.</p><p><strong>Specifying the strategy in the constraints module</strong></p><p>By default, the <code>GreaterThan</code> constraint uses the <code>transform</code> strategy. However, you can use the <code>handling_strategy</code> argument to control this. This argument accepts <code>'reject_sampling'</code> or <code>'transform'</code> as valid strategies.</p><pre><code class=\"language-python\">gt_reject_constraint = GreaterThan(\n  low='srch_ci',\n  high='srch_co',\n  handling_strategy='reject_sampling' # specify the strategy\n)</code></pre><p>Similar to before, we can then input this constraint into the model.</p><pre><code class=\"language-python\">np.random.seed(30)\n\n# Apply the constraint to the model\nmodel_with_reject_constraint = GaussianCopula(\n  primary_key='log_id',\n  constraints=[gt_reject_constraint])\n\nmodel_with_reject_constraint.fit(data)\n\n# Sample synthetic data\nconstrained_reject_data = model_with_reject_constraint.sample(500)\nconstrained_reject_data.head()</code></pre><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.40.44-PM.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"377\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/12/Screen-Shot-2021-12-20-at-3.40.44-PM.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/12/Screen-Shot-2021-12-20-at-3.40.44-PM.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/12/Screen-Shot-2021-12-20-at-3.40.44-PM.png 1600w, https://sdv.ghost.io/content/images/2021/12/Screen-Shot-2021-12-20-at-3.40.44-PM.png 2048w\" sizes=\"(min-width: 720px) 720px\"></figure><pre><code class=\"language-python\">invalid_rows = constrained_reject_data[constrained_reject_data['srch_ci'] &gt; constrained_reject_data['srch_co']\ninvalid_rows.shape[0]</code></pre><pre><code>0</code></pre><h3 id=\"what-other-deterministic-rules-are-already-available-in-sdv\">What other deterministic rules are already available in SDV?</h3><p>The <code>GreaterThan</code> constraint is one kind of deterministic rule, but there may be others that apply to your dataset. The SDV offers more constraints for other types of logic.</p><ul><li><a href=\"https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#unique-constraint\" rel=\"nofollow\">Unique</a> when values in a column must be unique to the entire dataset.</li><li><a href=\"https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#uniquecombinations-constraint\" rel=\"nofollow\">UniqueCombinations</a> to limit the permutations between multiple columns.</li><li><a href=\"https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#positive-and-negative-constraints\" rel=\"nofollow\">Positive and Negative</a> to enforce boundaries.</li><li><a href=\"https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#columnformula-constraint\" rel=\"nofollow\">ColumnFormula</a> when there is a formulaic association between columns.</li><li><a href=\"https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#rounding-constraint\" rel=\"nofollow\">Rounding</a> to enforce decimal precision.</li><li><a href=\"https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#between-constraint\" rel=\"nofollow\">Between</a> when one column's values must be between 2 other values.</li><li><a href=\"https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html#onehotencoding-constraint\" rel=\"nofollow\">OneHotEncoding</a> when your data includes a variable with one hot encoding.</li></ul><p>For each of them, you can specify handling strategies for <code>reject_sampling</code> to discard invalid data or <code>transform</code> to modify the data (unique to each constraint).</p><p><strong>What if my rule isn't included in the module?</strong></p><p>You may come across a rule that cannot be described by any of the constraints classes in the SDV. In this case, you can define a <a href=\"https://sdv.dev/SDV/user_guides/single_table/custom_constraints.html#defining-custom-constraints\" rel=\"nofollow\">CustomConstraint</a> with logic specific to your use case.</p><p>Additionally, consider <a href=\"https://github.com/sdv-dev/SDV/issues/new/choose\" rel=\"nofollow\"><strong>filing a feature request on GitHub</strong></a> with details about your use case &amp; scenario. We can add your logic as a pre-defined constraint so others can benefit from it too!</p><h3 id=\"takeaways\">Takeaways</h3><p>In this notebook, we explored what happens when we have a deterministic rule in our dataset.</p><ol><li>Machine learning models may not able to learn the deterministic rules out of the box, but it is possible to improve the model to learn these types of rules.</li><li>Deterministic rules can be handled by discarding invalid data (<strong>reject sampling</strong>) or by adding some clever preprocessing to your code (<strong>transforming</strong>).</li><li>The SDV offers a <code>constraints</code> module that allows you to input commonly found deterministic rules. You can specify the handling strategy for each constraint and apply multiple rules to the same dataset.</li></ol><p><strong>Further Reading</strong></p><p>For further information about constraints refer to the <a href=\"https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html\" rel=\"nofollow\">Handling Constraints User Guide</a>.</p>","url":"https://sdv.ghost.io/eng-sdv-constraints/","canonical_url":null,"uuid":"e4f85a3b-a4f0-4389-a9b8-f4d9faa60f74","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"61c10f636317ec003be8e39d","reading_time":8}}]}},"pageContext":{"slug":"engineering","pageNumber":0,"humanPageNumber":1,"skip":0,"limit":12,"numberOfPages":1,"previousPagePath":"","nextPagePath":""}},"staticQueryHashes":["2061773391","2358152166","2362887240","2439066133","2561578252","2657115718","2731221146","2839364760","4145280475"]}